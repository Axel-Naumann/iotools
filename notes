Formats:
  sqlite, Avro, Parquet, HDF5 row-wise, HD5 column-wise, ProtoBuf, Boost serialization?

Notes:
  Generate TTreeReaderValue variables?

CMS:
  50-100 GB flat ntuples
    flat because only first few jets interesting
    anyway needed for machine learning
  30+ branches

  10G data, rest MC (20% bigger per event)
  fast selection based on few branches

ATLAS:
  Numbers like CMS?
  Really flat?
  ntuples / scripts
  AFs?
  xAODs --> Derived AODs --> ntuples?
  PB        TB               GB
            3-35kb/ev (~20kb/ev)
            grid jobs / 1 day
            Not fully flat, std::vector

HDF5:
  Dataset (directories), data spaces (arrays)
  limited nesting
  file -- memory type mapping manual
  Cumbersome, multi-dimensional arrays and slicing (MPI)
  Compression: only on "closed chunks"

Avro:
  Looks nicely designed
  JSON parsing did not work
  ref counting, easy to make memory leaks
  deflate easy
  reading -- projections

Parquet:
  No documentation, fresh C++ lib ('14)
  Columns accessed by index (error-prone)
  Sizing of row groups manual
  Different compression algorithms
  Lots of dependencies (Thrift, Boost, arrow, brotli)
  LINKING MAKES UNZIPPING TWICE AS SLOW, use -lz -lparquet
  Cumbersome, individual column readers that move independently
  Dremel algorithm for nested columns (?)
  Compression per column
  Checksums

Protobuf:
  Not a file format, struct serialization
  Schema upgrade possible

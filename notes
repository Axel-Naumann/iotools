Formats:
  sqlite, Avro, Parquet, HDF5 row-wise, HD5 column-wise, ProtoBuf, Boost serialization?

Look at:
  file size,
  running time (warm cache, ssd, hdd)
  memory consumption
  read pattern

Notes:
  Generate TTreeReaderValue variables?

HDF5:
  Dataset (directories), data spaces (arrays)
  limited nesting
  file -- memory type mapping manual
  Cumbersome, multi-dimensional arrays and slicing (MPI)
  Compression: only on "closed chunks"

Avro:
  Looks nicely designed
  JSON parsing did not work
  ref counting, easy to make memory leaks
  deflate easy
  reading -- projections

Parquet:
  No documentation, fresh C++ lib ('14)
  Columns accessed by index (error-prone)
  Sizing of row groups manual
  Different compression algorithms
  Lots of dependencies (Thrift, Boost, arrow, brotli)
  LINKING MAKES UNZIPPING TWICE AS SLOW, use -lz -lparquet
  Cumbersome, individual column readers that move independently
  Dremel algorithm for nested columns (?)
  Compression per column
  Checksums

Protobuf:
  Not a file format, struct serialization
  Schema upgrade possible
